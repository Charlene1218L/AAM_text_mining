{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout, ConnectTimeout, ConnectionError, ProxyError, JSONDecodeError, ChunkedEncodingError\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "# 浏览器的推特高级搜索里设置好了以后，把搜索框的内容复制出来就行\n",
    "# 这种方式也可以爬用户推文，不过只能爬到原创推文(自己写的/回复别人的/转发方式是quote的这种加了自己的话的)，直接repost别人的推文的话爬不出来\n",
    "search_word = '\"urban air mobility\" lang:en until:2020-01-01 since:2017-01-01'\n",
    "csvname = 'uam20230631.csv'  # 输出的文件名\n",
    "cursor_txtname = f'cursor {csvname[:-4]}.txt'  # 这个文件等爬完了再删。用来记录爬到哪一页的，如果断掉的话可以重新运行原地继续\n",
    "print('csvname:', csvname)\n",
    "product = 'Latest'  # 按时间倒序\n",
    "# 还要填一下下面的cookies\n",
    "\n",
    "column_user_headers = ['博主昵称', '博主账号', '博主ID号', '加入推特时间', '博主位置', '个人简介', '账户验证类型',\n",
    "                       '博主粉丝数', '博主关注人数', '博主发博数', '博主上传多媒体数目', '博主获点赞数', '博主收藏数']\n",
    "column_info_headers = ['推文链接', '推文ID', '发布时间', '转载自', '回复于', '观看数', '点赞数', '收藏数', '评论数', '转发数', '引用数',\n",
    "                       '文字内容', '媒体类型', '封面图', '视频时长（秒）', '视频链接']\n",
    "column_headers = column_user_headers + column_info_headers\n",
    "column_headers += ['引用链接']\n",
    "df_out = pd.DataFrame(columns=column_headers)\n",
    "\n",
    "if os.path.exists(csvname):\n",
    "    if input(f'\"{csvname}\" 已存在，续写(1)/删除(2)？').strip() == \"2\":\n",
    "        os.remove(csvname)\n",
    "if not os.path.exists(csvname):\n",
    "    if os.path.exists(cursor_txtname):\n",
    "        os.remove(cursor_txtname)\n",
    "\n",
    "if not os.path.exists(csvname):\n",
    "    df_out.to_csv(csvname, mode='w', header=True, index=False, encoding='utf-8-sig')\n",
    "    stored_data = []\n",
    "else:\n",
    "    df = pd.read_csv(csvname, encoding='utf-8-sig')\n",
    "    print('df.shape:', df.shape)\n",
    "    stored_data = df['推文链接'].tolist()\n",
    "count = len(stored_data)\n",
    "print('count =', count)\n",
    "\n",
    "sleep_sec_min = 8\n",
    "sleep_sec_max = 12\n",
    "print()\n",
    "\n",
    "\n",
    "class IterCookieHeader:\n",
    "    def __init__(self):\n",
    "        self.list_cookies = [  # auth_token 是每个账号固定的，不改密码不改网络环境的话一般不会变\n",
    "            {\n",
    "                'auth_token': '82c97ccc8d9dd4418d83f8e4b7c28ad724657bce'\n",
    "            },\n",
    "            {\n",
    "                'auth_token': '020a715d6bc2fef0959f5d42cf17abcd18b650a3'\n",
    "            },\n",
    "            #{\n",
    "            #    'auth_token': '353a0f99f87408b2bdd7b4ba14fcf1304aef3a96'\n",
    "            #},\n",
    "            \n",
    "            # 还有推特账号的话可以继续添加\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        print('更新ct0中...')\n",
    "        for i in range(len(self.list_cookies)):\n",
    "            while True:\n",
    "                ses = requests.session()\n",
    "                r_ct0 = None\n",
    "                try:\n",
    "                    params_ct0 = {\n",
    "                        'prefetchTimestamp': str(int(time.time() * 1000)),\n",
    "                    }\n",
    "\n",
    "                    r_ct0 = ses.get('https://twitter.com/home', params=params_ct0, cookies=self.list_cookies[i],\n",
    "                                     verify=False)\n",
    "                    cookies_ses = requests.utils.dict_from_cookiejar(ses.cookies)\n",
    "                    ct0 = cookies_ses['ct0']\n",
    "                    self.list_cookies[i]['ct0'] = ct0\n",
    "                    print(i, ct0)\n",
    "                    break\n",
    "                except (ProxyError, ConnectionError, ConnectTimeout, ReadTimeout, ChunkedEncodingError) as e:\n",
    "                    print('r_ct0 ConnectError:', e)\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                except KeyError:\n",
    "                    print(f'账号auth_token = {self.list_cookies[i][\"auth_token\"]} 可能已失效，请检查替换后重新运行程序。')\n",
    "                    input(0)\n",
    "        self.headers = {\n",
    "            'authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA',\n",
    "        }\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iter_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.iter_index += 1\n",
    "\n",
    "        if self.iter_index >= len(self.list_cookies):\n",
    "            self.iter_index = 0\n",
    "        cookie = self.list_cookies[self.iter_index]\n",
    "        self.headers['x-csrf-token'] = cookie['ct0']\n",
    "        return cookie, self.headers\n",
    "\n",
    "\n",
    "cookies_header_iter = iter(IterCookieHeader())\n",
    "cookies, headers = next(cookies_header_iter)\n",
    "\n",
    "\n",
    "def get_user_detail(json_user):\n",
    "    user_id = json_user['rest_id']\n",
    "    screen_name = json_user['legacy'][\"screen_name\"]\n",
    "    nickname = json_user['legacy'][\"name\"]\n",
    "    user_created_at = json_user['legacy']['created_at']\n",
    "    try:\n",
    "        location = json_user['legacy']['location']\n",
    "    except KeyError:\n",
    "        location = '--'\n",
    "    description = json_user['legacy']['description']\n",
    "    try:\n",
    "        verified_type = json_user['legacy']['verified_type']\n",
    "    except KeyError:\n",
    "        verified_type = '--'\n",
    "    follower_count = json_user['legacy'][\"followers_count\"]\n",
    "    following_count = json_user['legacy'][\"friends_count\"]\n",
    "    tweet_count = json_user['legacy']['statuses_count']\n",
    "    media_count = json_user['legacy']['media_count']  # 发布的多媒体文件数\n",
    "    favourites_count = json_user['legacy'][\"favourites_count\"]\n",
    "    listed_count = json_user['legacy'][\"listed_count\"]\n",
    "    #print(nickname, screen_name, user_id)##11\n",
    "    return [nickname, screen_name, user_id, user_created_at, location, description, verified_type, follower_count, following_count, tweet_count, media_count,\n",
    "            favourites_count, listed_count]\n",
    "\n",
    "\n",
    "def get_tweet_detail(entry_content_itemContent, by=''):\n",
    "    global stored_data, current_date\n",
    "    if by == 'tweetId':\n",
    "        tweetResult = 'tweetResult'\n",
    "    elif by == 'quote':\n",
    "        tweetResult = 'quoted_status_result'\n",
    "    else:\n",
    "        tweetResult = 'tweet_results'\n",
    "    result = entry_content_itemContent[tweetResult]['result']\n",
    "    try:\n",
    "        tweet_id = result['rest_id']\n",
    "    except KeyError:\n",
    "        try:\n",
    "            tweet_id = result['tweet']['rest_id']\n",
    "        except KeyError:\n",
    "            return None\n",
    "        legacy = result['tweet']['legacy']\n",
    "        user_result = result['tweet']['core']['user_results']['result']\n",
    "    else:\n",
    "        legacy = result['legacy']\n",
    "        user_result = result['core']['user_results']['result']\n",
    "    list_user_info = get_user_detail(user_result)\n",
    "    screen_name = user_result['legacy'][\"screen_name\"]\n",
    "    tweet_url = f'https://twitter.com/{screen_name}/status/{tweet_id}'\n",
    "    tweet_created_at = legacy['created_at']\n",
    "    if by != 'quote' and by != 'tweetId':\n",
    "        current_date = tweet_created_at\n",
    "    if tweet_url in stored_data and by != 'quote':\n",
    "        return None\n",
    "    else:\n",
    "        stored_data.append(tweet_url)\n",
    "    try:\n",
    "        text = result['note_tweet']['note_tweet_results']['result']['text']\n",
    "    except KeyError:\n",
    "        text = legacy['full_text']\n",
    "    #print(text)##33\n",
    "    try:\n",
    "        retweeted_status_result = legacy['retweeted_status_result']\n",
    "        screen_name_owner, text = re.findall(r'^RT @(\\w*): ([\\S\\s]*)$', text)[0]\n",
    "    except KeyError:\n",
    "        screen_name_owner = ''\n",
    "    try:\n",
    "        in_reply_to_screen_name = legacy['in_reply_to_screen_name']\n",
    "    except KeyError:\n",
    "        in_reply_to_screen_name = ''\n",
    "    if not screen_name_owner:\n",
    "        try:\n",
    "            view_count = result['views']['count']\n",
    "        except KeyError:\n",
    "            view_count = '--'\n",
    "        favorite_count = legacy['favorite_count']\n",
    "        bookmark_count = legacy['bookmark_count']\n",
    "        reply_count = legacy['reply_count']\n",
    "        try:\n",
    "            retweet_count = legacy['retweet_count']\n",
    "        except KeyError:\n",
    "            retweet_count = '--'\n",
    "        quote_count = legacy['quote_count']\n",
    "    else:\n",
    "        try:\n",
    "            result = legacy['retweeted_status_result']['result']['tweet']\n",
    "        except KeyError:\n",
    "            result = legacy['retweeted_status_result']['result']\n",
    "        legacy = result['legacy']\n",
    "        try:\n",
    "            view_count = result['count']\n",
    "        except KeyError:\n",
    "            view_count = '--'\n",
    "        favorite_count = legacy['favorite_count']\n",
    "        bookmark_count = legacy['bookmark_count']\n",
    "        reply_count = legacy['reply_count']\n",
    "        retweet_count = legacy['retweet_count']\n",
    "        quote_count = legacy['quote_count']\n",
    "    output_line = [tweet_url, tweet_id, tweet_created_at, screen_name_owner, in_reply_to_screen_name, view_count, favorite_count, bookmark_count, reply_count,\n",
    "                   retweet_count, quote_count, text]\n",
    "    try:\n",
    "        media = legacy['extended_entities']['media']\n",
    "    except KeyError:\n",
    "        media_type = 'text'\n",
    "        output_line += [media_type, '', '', '']\n",
    "    else:\n",
    "        media_type = media[0]['type']\n",
    "        if media_type == 'video':\n",
    "            duration_sec = media[0]['video_info']['duration_millis'] / 1000\n",
    "            cover_img_url = media[0]['media_url_https']\n",
    "            video_sources = media[0]['video_info']['variants']\n",
    "            video_url = ''\n",
    "            for video_source in video_sources:\n",
    "                if video_source['content_type'] == 'video/mp4':\n",
    "                    video_url = video_source['url']\n",
    "                    break\n",
    "            output_line += [media_type, cover_img_url, duration_sec, video_url]\n",
    "        elif media_type == 'photo':\n",
    "            img_url_list = []\n",
    "            for medium in media:\n",
    "                img_url = medium['media_url_https']\n",
    "                img_url_list.append(img_url)\n",
    "            output_line += [media_type, str(img_url_list), '', '']\n",
    "        elif media_type == 'animated_gif':\n",
    "            cover_img_url = media[0]['media_url_https']\n",
    "            video_sources = media[0]['video_info']['variants']\n",
    "            video_url = ''\n",
    "            for video_source in video_sources:\n",
    "                if video_source['content_type'] == 'video/mp4':\n",
    "                    video_url = video_source['url']\n",
    "                    break\n",
    "            output_line += [media_type, cover_img_url, '', video_url]\n",
    "        else:\n",
    "            print(media_type)\n",
    "            input('***???***')\n",
    "            output_line += ['', '', '']\n",
    "    if by == 'quote':\n",
    "        return list_user_info + output_line\n",
    "    else:\n",
    "        try:\n",
    "            quoted_link = legacy['quoted_status_permalink']['expanded']\n",
    "        except KeyError:\n",
    "            quoted_link = ''\n",
    "        return list_user_info + output_line + [quoted_link]\n",
    "\n",
    "\n",
    "def get_new_search_word(search_word, date_first):\n",
    "    if not hasattr(get_new_search_word, 'search_word'):\n",
    "        get_new_search_word.search_word = search_word\n",
    "    if not hasattr(get_new_search_word, 'null_count'):\n",
    "        get_new_search_word.null_count = 0\n",
    "    print('null_count', get_new_search_word.null_count)\n",
    "    print('old search_word:', get_new_search_word.search_word)\n",
    "    d_first = date_first.strftime(\"%Y-%m-%d\")\n",
    "    try:\n",
    "        d_end = re.findall(r'until:(\\d{4}-\\d{2}-\\d{2})', search_word)[0]\n",
    "        if d_end <= d_first:\n",
    "            d_first = (datetime.datetime.strptime(d_end, \"%Y-%m-%d\") - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            get_new_search_word.null_count += 1\n",
    "        else:\n",
    "            get_new_search_word.null_count = 0\n",
    "        if get_new_search_word.null_count >= 3:\n",
    "            return None\n",
    "        search_word = search_word.replace(f'until:{d_end}', f'until:{d_first}')\n",
    "    except IndexError:\n",
    "        search_word = search_word + f' until:{d_first}'\n",
    "    print('new search_word:', search_word)\n",
    "    get_new_search_word.search_word = search_word\n",
    "    return search_word\n",
    "\n",
    "\n",
    "search_word = search_word.replace('\"', '\\\\\"')\n",
    "\n",
    "params_search = {\n",
    "    'features': '{\"rweb_lists_timeline_redesign_enabled\":true,\"responsive_web_graphql_exclude_directive_enabled\":true,\"verified_phone_label_enabled\":false,\"creator_subscriptions_tweet_preview_api_enabled\":true,\"responsive_web_graphql_timeline_navigation_enabled\":true,\"responsive_web_graphql_skip_user_profile_image_extensions_enabled\":false,\"tweetypie_unmention_optimization_enabled\":true,\"responsive_web_edit_tweet_api_enabled\":true,\"graphql_is_translatable_rweb_tweet_is_translatable_enabled\":true,\"view_counts_everywhere_api_enabled\":true,\"longform_notetweets_consumption_enabled\":true,\"tweet_awards_web_tipping_enabled\":false,\"freedom_of_speech_not_reach_fetch_enabled\":true,\"standardized_nudges_misinfo\":true,\"tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled\":false,\"longform_notetweets_rich_text_read_enabled\":true,\"longform_notetweets_inline_media_enabled\":false,\"responsive_web_enhance_cards_enabled\":false}',\n",
    "}\n",
    "\n",
    "if not os.path.exists(cursor_txtname):\n",
    "    if count > 0:\n",
    "        date_first = datetime.datetime.strptime(df['发布时间'].tolist()[-1], \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "        search_word = get_new_search_word(search_word, date_first)\n",
    "        if not search_word:\n",
    "            print(f'{search_word} 已爬完。')\n",
    "            exit(0)\n",
    "    params_search['variables'] = f'{{\"rawQuery\":\"{search_word}\",\"count\":20,\"querySource\":\"typed_query\",\"product\":\"{product}\"}}'\n",
    "else:\n",
    "    cursor_txt = open(cursor_txtname, 'r', encoding='utf-8-sig').read().split('\\n')\n",
    "    search_word, cursor = cursor_txt[0], cursor_txt[1]\n",
    "    params_search[\n",
    "        'variables'] = f'{{\"rawQuery\":\"{search_word}\",\"count\":20,\"cursor\":\"{cursor}\",\"querySource\":\"typed_query\",\"product\":\"{product}\"}}'\n",
    "\n",
    "current_date = None  # 初始化一下，函数里定义\n",
    "while True:  # 改写search_word中的日期的循环\n",
    "    print(params_search['variables'])\n",
    "    while True:  # 同一个search_word下各个页面的循环\n",
    "        sleep_sec = round(random.uniform(sleep_sec_min, sleep_sec_max))\n",
    "        print(f'time.sleep({sleep_sec})')\n",
    "        time.sleep(sleep_sec)\n",
    "        while True:  # requests防报错链接的循环\n",
    "            r_search = None\n",
    "            try:\n",
    "                r_search = requests.get(\n",
    "                    'https://twitter.com/i/api/graphql/IOJ89SDQ9IrZ2t7hSD4Fdg/SearchTimeline',\n",
    "                    params=params_search, cookies=cookies, headers=headers,\n",
    "                    timeout=(10, 10), verify=False)\n",
    "                instructions = r_search.json()['data']['search_by_raw_query']['search_timeline']['timeline']['instructions']\n",
    "                break\n",
    "            except (ProxyError, ConnectionError, ConnectTimeout, ReadTimeout, ChunkedEncodingError) as e:\n",
    "                print('r_search ConnectError:', e)\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except (JSONDecodeError, KeyError) as e:\n",
    "                print('r_search JsonError:', e)\n",
    "                print(r_search.text)\n",
    "                if 'Rate limit exceeded' in r_search.text:\n",
    "                    cookies, headers = next(cookies_header_iter)\n",
    "                    sleep_sec += 5\n",
    "                    print(cookies)\n",
    "                    print(f'time.sleep({sleep_sec})')\n",
    "                    time.sleep(sleep_sec)\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "        with open(f'r_search.json', 'w', encoding='utf-8-sig') as f:\n",
    "            f.write(r_search.text)\n",
    "\n",
    "        cursor = ''\n",
    "        if len(instructions) == 0:\n",
    "            print(\"len(instructions)==0\")\n",
    "            break\n",
    "\n",
    "        n = 0\n",
    "        for instruction in instructions:\n",
    "            if instruction['type'] == \"TimelineAddEntries\":\n",
    "                timeline_tweets = instruction['entries']\n",
    "                if len(timeline_tweets) == 0:\n",
    "                    print(\"len(timeline_tweets)==0\")\n",
    "                    break\n",
    "                for tweet in timeline_tweets:\n",
    "                    if tweet['entryId'].startswith('tweet-'):\n",
    "                        n += 1\n",
    "                        linelist_tweet_detail = get_tweet_detail(tweet['content']['itemContent'])\n",
    "                        if linelist_tweet_detail:\n",
    "                            count += 1\n",
    "                            df_out.loc[0] = np.array(linelist_tweet_detail, dtype=object)\n",
    "                            try:\n",
    "                                df_out.to_csv(csvname, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "                            except Exception as e:\n",
    "                                if 'need to escape, but no escapechar set' in str(e):\n",
    "                                    with open('escapechar.json', 'a', encoding='utf-8-sig') as f:\n",
    "                                        f.write(json.dumps(df_out.loc[0].tolist()) + '\\n')\n",
    "                    elif tweet['entryId'].startswith('profile-conversation-'):\n",
    "                        n += 1\n",
    "                        for item in tweet['content']['items']:\n",
    "                            linelist_tweet_detail = get_tweet_detail(item['item']['itemContent'])\n",
    "                            print(linelist_tweet_detail)\n",
    "                            if linelist_tweet_detail:\n",
    "                                count += 1\n",
    "                                df_out.loc[0] = np.array(linelist_tweet_detail, dtype=object)\n",
    "                                try:\n",
    "                                    df_out.to_csv(csvname, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "                                except Exception as e:\n",
    "                                    if 'need to escape, but no escapechar set' in str(e):\n",
    "                                        with open('escapechar.json', 'a', encoding='utf-8-sig') as f:\n",
    "                                            f.write(json.dumps(df_out.loc[0].tolist()) + '\\n')\n",
    "                    elif any(tweet['entryId'].startswith(x) for x in ['who-to-follow-', 'cursor-top-', 'toptabsrpusermodule-', 'relevanceprompt-']):\n",
    "                        continue\n",
    "                    elif tweet['entryId'].startswith('cursor-bottom-'):\n",
    "                        cursor = tweet['content']['value']  # 第一次搜索走这里，后面靠cursor的走创建2\n",
    "                        print('cursor:', cursor)\n",
    "                        if n > 0:\n",
    "                            open(cursor_txtname, 'w', encoding='utf-8-sig').write(search_word + '\\n' + cursor)\n",
    "                        print(cursor_txtname, '已创建1')\n",
    "                        params_search[\n",
    "                            'variables'] = f'{{\"rawQuery\":\"{search_word}\",\"count\":20,\"cursor\":\"{cursor}\",\"querySource\":\"typed_query\",\"product\":\"{product}\"}}'\n",
    "                        print('--------------------------')\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"tweet['entryId'] =\", tweet['entryId'])\n",
    "                        open(f'r_search_{search_word}_{count}.json', 'w', encoding='utf-8-sig').write(r_search.text)\n",
    "                        input(f'r_search_{search_word}_{count}.json')\n",
    "                break\n",
    "        else:\n",
    "            print('Not exist: TimelineAddEntries')\n",
    "            break\n",
    "        for instruction in instructions:\n",
    "            if instruction['type'] == \"TimelineReplaceEntry\":\n",
    "                if instruction['entry_id_to_replace'].startswith('cursor-bottom-'):\n",
    "                    print(instruction['entry_id_to_replace'])\n",
    "                    cursor = instruction['entry']['content']['value']\n",
    "                    print('cursor:', cursor)\n",
    "                    open(cursor_txtname, 'w', encoding='utf-8-sig').write(search_word + '\\n' + cursor)\n",
    "                    print(cursor_txtname, '已创建2')\n",
    "                    params_search[\n",
    "                        'variables'] = f'{{\"rawQuery\":\"{search_word}\",\"count\":20,\"cursor\":\"{cursor}\",\"querySource\":\"typed_query\",\"product\":\"{product}\"}}'\n",
    "                    print('--------------------------')\n",
    "                    break\n",
    "        print('n =', n)\n",
    "        print('count =', count)\n",
    "        print('Current Date:', current_date)\n",
    "        if len(timeline_tweets) == 0:\n",
    "            print(\"len(timeline_tweets)==0\")\n",
    "            break\n",
    "        if cursor == '':\n",
    "            print(\"cursor==''\")\n",
    "            break\n",
    "\n",
    "    df = pd.read_csv(csvname, encoding='utf-8-sig')\n",
    "    if len(df) == 0:\n",
    "        print('No search result.')\n",
    "        break\n",
    "    date_first = datetime.datetime.strptime(df['发布时间'].tolist()[-1], \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    search_word = get_new_search_word(search_word, date_first)\n",
    "    if not search_word:\n",
    "        break\n",
    "    params_search['variables'] = f'{{\"rawQuery\":\"{search_word}\",\"count\":20,\"querySource\":\"typed_query\",\"product\":\"{product}\"}}'\n",
    "\n",
    "print(f'\"{csvname}\" 保存成功。')\n",
    "df = pd.read_csv(csvname, encoding='utf-8-sig')\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
