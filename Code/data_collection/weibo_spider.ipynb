{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "k='2023-06-07'\n",
    "j='2023-12-31'\n",
    "dataa=[]\n",
    "import datetime\n",
    "dates = []\n",
    "dt = datetime.datetime.strptime(k, \"%Y-%m-%d\")\n",
    "date = k[:]\n",
    "while date <= j:\n",
    "    dates.append(date)\n",
    "    dt = dt + datetime.timedelta(1)\n",
    "    date = dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "def ua():\n",
    "\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11', \n",
    "        'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',\n",
    "        'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "        'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',\n",
    "        'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7',\n",
    "        'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0',\n",
    "\n",
    "    ]\n",
    "\n",
    "    agent = random.choice(user_agents)\n",
    "\n",
    "    return agent\n",
    "\n",
    "for wor in ['低空经济']:\n",
    "    for tii in dates: \n",
    "        print(tii)\n",
    "        for i in range(1, 51):\n",
    "            url = f'https://s.weibo.com/weibo?q={wor}&typeall=1&suball=1&timescope=custom:{tii}-0:{tii}-23&Refer=g&page={i}'            \n",
    "            #url\n",
    "            headers = {\n",
    "                'cookie':'SCF=ApWp3d4Y88M0WvYCVhkgsasd3T9bHbfPd9ad5Z1MGFxf-8CZyA0z_OD2_rShrz3VDXD61UDIifvttqZ3JQ9jiw8.; SUB=_2A25KD6MfDeRhGeFG6FYR8S7Owz-IHXVpZLrXrDV6PUJbktANLXGgkW1NeauaUIoKFMoXds9-to37Hq8tt1vd31yd; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFaFL9wsf1sc9812OKZcN525JpX5KMhUgL.FoMRe0B7eK5E1he2dJLoI0YLxKnL1h2L1-2LxKnLBoBLBKBLxKqLBK-LB.eLxKML1-2L1hBLxKML1heLBKMLxK.LBo.L1h-LxK.LBK-LB-Bt; ALF=1731420239; _T_WM=97887518987; XSRF-TOKEN=e164cb; WEIBOCN_FROM=1110005030; mweibo_short_token=a18e50b6d8; MLOGIN=1',\n",
    "                'user-agent': 'Chrome/98.0.4758.102 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            sleep(1)\n",
    "            while 1:\n",
    "                try:\n",
    "                    html0 = requests.get(url=url, headers=headers).text\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            t = BeautifulSoup(html0, 'html.parser')\n",
    "\n",
    "            card = t.find_all('div', class_=\"card-wrap\")\n",
    "            print(len(card))\n",
    "            \n",
    "            try:\n",
    "                pd1 = t.find('div', class_=\"card card-no-result s-pt20b40\")\n",
    "                if pd1!=None:\n",
    "                    continue\n",
    "                    #break\n",
    "            except:\n",
    "                pd1=''\n",
    "            for p in card:\n",
    "                dic={}\n",
    "                if p.find_all('p', class_=\"txt\")!=[]:\n",
    "                    \n",
    "                    try:\n",
    "                        dic['word']=wor\n",
    "                        yh_url='https:'+p.find('div',class_=\"avator\").a['href']\n",
    "                        dic['user_url']=yh_url\n",
    "                        bw_url='https:'+p.find('div', class_=\"from\").a['href']\n",
    "                        \n",
    "                        dic['url']=bw_url\n",
    "                        yh_id=bw_url.split('/')[3]\n",
    "                        dic['user_id']=yh_id\n",
    "                        bo_id=bw_url.split('/')[4].replace('?refer_flag=1001030103_','')\n",
    "                        dic['id']=bo_id\n",
    "                        id=yh_id+'_'+bo_id\n",
    "                        dic['_id']=id\n",
    "                        txt=p.find_all('p', class_=\"txt\")[-1].find_all('a')\n",
    "                        dic['place']='无'\n",
    "                        \n",
    "                        for tx in txt:\n",
    "                            try:\n",
    "                                if tx.find('i',class_=\"wbicon\").text=='2':\n",
    "                                    dic['place']=str(tx.text).replace('2','')\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        dic['content'] = str(p.find_all('p', class_=\"txt\")[-1].text).replace('\\n', '').replace('\\u200b','').replace('收起d','')\n",
    "                        dic['comment']= str(p.find_all('a', class_=\"woo-box-flex woo-box-alignCenter woo-box-justifyCenter\")[-2].text).replace(' ','')\n",
    "                        if dic['comment']== '评论':\n",
    "                            dic['comment']= 0\n",
    "                        dic['name']= p.find('a', class_=\"name\").text\n",
    "                        time = str(p.find('div', class_=\"from\").text).replace(' ', '').replace('\\n', '').replace('\\xa0','')\n",
    "                        time = time + '来自0'\n",
    "                        dic['pubtime']= time.split('来自')[0]\n",
    "                        dic['tools']=time.split('来自')[1]\n",
    "                        dic['like']= str(p.find_all('span', class_=\"woo-like-count\")[-1].text).replace(' ', '')                  \n",
    "                        if dic['like']== '赞':\n",
    "                            dic['like']= 0\n",
    "                        dic['transfer']= str(p.find_all('a', class_=\"woo-box-flex woo-box-alignCenter woo-box-justifyCenter\")[-3].text).replace(' ', '')\n",
    "                        \n",
    "                        if dic['transfer']== '转发':\n",
    "                            dic['transfer']= 0\n",
    "                        yhurl = f'https://weibo.com/ajax/profile/info?uid={yh_id}'\n",
    "                        while 1:\n",
    "                            try:\n",
    "                                sleep(1)\n",
    "                                uhht = requests.get(url=yhurl, headers=headers).json()\n",
    "                                break\n",
    "                            except:\n",
    "                                pass\n",
    "                        dic['num_follows'] = uhht['data']['user']['friends_count']\n",
    "                        dic['num_fans'] = uhht['data']['user']['followers_count']\n",
    "                        dic['num_tweets'] = uhht['data']['user']['statuses_count']\n",
    "                        xb = uhht['data']['user']['gender']\n",
    "\n",
    "                        if xb == 'f':\n",
    "                            dic['gender'] = '女'\n",
    "                        else:\n",
    "                            dic['gender'] = '男'\n",
    "                        dic['location'] = uhht['data']['user']['location']\n",
    "                        dic['verified']= uhht['data']['user']['verified']\n",
    "                        dic['verified_type']= uhht['data']['user']['verified_type']\n",
    "                        try:\n",
    "                            dic['verified_reason'] = uhht['data']['user']['verified_reason']\n",
    "                        except:\n",
    "                            dic['verified_reason'] =''\n",
    "                            pass\n",
    "                        \n",
    "                        \n",
    "                        urls = f'https://weibo.com/ajax/profile/detail?uid={yh_id}'\n",
    "                        while 1:\n",
    "                            try:\n",
    "                                sleep(1)\n",
    "                                res = requests.get(url=urls, headers=headers).json()\n",
    "                                break\n",
    "                            except:\n",
    "                                pass\n",
    "                        dic['birthday'] = res['data']['birthday']\n",
    "                        dic['ip']=''\n",
    "                        try:\n",
    "                            dic['ip'] = str(res['data']['ip_location']).replace('IP属地：', '')\n",
    "                        except:\n",
    "                            pass\n",
    "                        fp = open(f\"D:\\\\OneDrive\\\\UAM\\\\Data\\\\低空经济\\\\低空经济_2023.csv\", 'a+', encoding='utf-8-sig', newline='')\n",
    "                        csv_fp = csv.writer(fp)\n",
    "                        csv_fp.writerow(dic.values())\n",
    "                        fp.close()\n",
    "                        print(f\"Writting successfully：{dic}\")\n",
    "                    except:\n",
    "                        print(i,\"Failed\")\n",
    "                        pass\n",
    "                    \n",
    "print(\"Finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
