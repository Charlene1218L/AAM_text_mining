{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'../Data/input/Twitter_uncleaned_data.csv')\n",
    "# Extract posting year\n",
    "data['time']= pd.to_datetime(data['发布时间'],format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "data['year'] = data['time'].year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of duplicate data\n",
    "data=data.drop_duplicates(subset=['推文链接'],keep='last')\n",
    "# Lowercase the data\n",
    "data['text_cleaned'] = data['文字内容'].apply(lambda x: x.lower())\n",
    "# Filtering data containing related words\n",
    "keywords = ['urban air mobility', 'uam ','air taxi','evtol','urban aerial mobility','drone taxi','autonomous air vehicle','vtol','flying car','mobility','uav','advenced air','eve','ehang','jaunt','joby','aviation','volocopter']\n",
    "data = data[data[\"text_cleaned\"].str.contains('|'.join(keywords))]\n",
    "# Remove useless characters from text\n",
    "data['text_cleaned'] = data['文字内容'].str.replace('\\n', ' ')\n",
    "data['text_cleaned'] = data['text_cleaned'].apply(lambda x: re.sub(r'@(\\w+)', '', x))\n",
    "data['text_cleaned'] = data['text_cleaned'].apply(lambda x: re.sub(r'#(\\w+)', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword \"VTOL\" data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vtol=data[data['关键词']=='vtol']\n",
    "keywords = ['enemy','wars',' war','war ','$vtol','army','weapon','battle','military','soldier']\n",
    "data_vtol=data_vtol[-(data_vtol[\"text_cleaned\"].str.contains('|'.join(keywords)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[-(data['关键词']=='vtol')]\n",
    "data=pd.concat(data,data_vtol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nation identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call openai to recognize the country\n",
    "openai.api_key = \"sk-uZNblLpNRmFyvzStF39f4d9d5353430a8792E1134bB95f24\"\n",
    "openai.base_url = \"https://api.gpt.ge/v1/\"\n",
    "openai.default_headers = {\"x-foo\": \"true\"}\n",
    "\n",
    "def askquestion(location):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Please identify the full name of the country represented by each element of the list (location or latitude/longitude) and return a list of the same length, making sure to follow the format ['xxx','xxx','xxx'] strictly, or answer unkonwn if it's not clear, and also answer unkonwn if there are more than one country contained in a single element\"},\n",
    "            {\"role\": \"user\", \"content\": location},\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean nation text\n",
    "data['location']=data[\"博主位置\"].copy()\n",
    "data['location'].fillna('null', inplace=True)\n",
    "filtered_rows = data['location'].str.match(r'^[0-9. +-]+$')\n",
    "data.loc[filtered_rows, 'location'] = 'useless'\n",
    "keywords = ['World', '.com','world','earth','Global','global','Earth']\n",
    "filtered_rows =data['location'].str.contains('|'.join(keywords), case=False)\n",
    "data.loc[filtered_rows, 'location'] = 'useless'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all=[]\n",
    "a=10\n",
    "\n",
    "for i in range(0, int(len(data)/a)):\n",
    "    tag=True\n",
    "    count=0\n",
    "    while tag and count<=5:\n",
    "        try:\n",
    "            dic={}\n",
    "            dic['No.']=i*a\n",
    "            # 删除每个元素中的符号\n",
    "            input_list=data['location'].iloc[i*a:i*a+a].tolist()\n",
    "            cleaned_list = [address.replace(\"'\", '') for address in input_list]\n",
    "            input_location = str(cleaned_list)\n",
    "            dic['input']=input_location\n",
    "            dic['input_count']=len(input_list)\n",
    "            nation=askquestion(input_location)\n",
    "            dic['output']=nation\n",
    "            output_nation = ast.literal_eval(nation)    # 将文本转换为列表\n",
    "            dic['output_count']=len(output_nation)\n",
    "            print(str(i)+'/'+str(len(data)/a))\n",
    "            fp = open(f\"./Data/interim/twitter_nation.csv\", 'a+', encoding='utf-8-sig', newline='')\n",
    "            csv_fp = csv.writer(fp)\n",
    "            csv_fp.writerow(dic.values())\n",
    "            fp.close()\n",
    "            tag=False\n",
    "        except Exception as e:\n",
    "            print(str(i)+'出错了')\n",
    "            tag=True\n",
    "            count=count+1\n",
    "\n",
    "# After calling openai for recognition, some recognition errors need to be manually checked and corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nation_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nation=pd.read_csv(r'../Data/interim/twitter_nation.csv')\n",
    "merged_list = []\n",
    "data_nation['output_list'].apply(lambda y:merged_list.extend(ast.literal_eval(y)))\n",
    "my_series = pd.Series(merged_list)\n",
    "nation_list = pd.DataFrame()\n",
    "nation_list['nation'] = my_series\n",
    "# Add nation to all twitter data\n",
    "data_final=pd.concat([data,nation_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the data\n",
    "data_final['nation_cleaned'] = data_final['nation'].apply(lambda x: x.lower())\n",
    "#nation clean\n",
    "data_final['nation_cleaned'].replace(['undefined','country unknown','erusea',' unknown','africa','greenland','unbekannt','unnonwn','bikini bottom','international waters','open skies','sky','belka','lesotho','east timor','international space station','western sahara','guam','un','aus','uam','eu','udknown','metaverse','known','undef','north'], 'unknown', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['undefined','unkonwn','international','european union','unkown', 'unnown','europe','mars','unavailable','antarctica','earth','unknonwn','unknwon','asia','unlnown','unknonw','multiple countries','unidentified','world'], 'unknown', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['united state',' united states','san francisco','united states ','wisconsin','new jersey','united states.','northern mariana islands','us','washington','usa','united sates','united states of america','puerto rico','united states and united kingdom','texas','new hampshire','north carolina','united states virgin islands'], 'united states', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['montserrat','falkland islands',' united kingdom','england','uk','scotland','isle of man','wales','gibraltar','guernsey','jersey','cayman islands','anguilla','saint helena'], 'united kingdom', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['hong kong','taiwan','macau','macao','peoples republic of china'], 'china', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['vatican city','vatican'], 'holy see', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['the netherlands','nederland','aruba','saint martin','sint maarten','south netherlands','Curaçao','curaçao'], 'netherlands', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['republic of korea'], 'south korea', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['kosovo'], 'serbia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['new caledonia'], 'france', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['democratic republic of the congo','republic of the congo'], 'congo', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['republic of the philippines'], 'philippines', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['the bahamas'], 'bahamas', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['brasil'], 'brazil', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['republic of croatia'], 'croatia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['islamic republic of iran'], 'iran', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['canada and united states','canada '], 'canada', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['ivory coast'], \"côte d'ivoire\", inplace=True)\n",
    "data_final['nation_cleaned'].replace(['kingdom of morocco'], 'morocco', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['russian federation'], 'russia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['lebannon'], 'Lebanon', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['kingdom of saudi arabia'], 'saudi arabia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['federated states of micronesia'], 'micronesia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['germany '], 'germany', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['islamic republic of pakistan'], 'pakistan', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('../Data/interim/twitter_nation_cleaned.csv', encoding=\"utf_8_sig\",index=True)\n",
    "# After the initial cleaning of the nation text using the code, there were some minor errors that needed to be cleaned manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retain useful data for next statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'./Data/interim/twitter_nation_cleaned.csv')\n",
    "data_retained = data[['关键词', '推文链接', '博主ID号','账户验证类型','nation_cleaned','year','text_cleaned']]\n",
    "useful_data = pd.DataFrame(data_retained)\n",
    "# Regularization of column names\n",
    "new_column_names = {'关键词': 'keyword', '推文链接': 'link', '博主ID号': 'user_id', '账户验证类型': 'user_type', 'nation_cleaned': 'nation', 'year': 'year', 'text_cleaned': 'text_cleaned'}\n",
    "useful_data = useful_data.rename(columns=new_column_names)\n",
    "useful_data.to_csv('../Data/interim/twitter_text.csv', encoding=\"utf_8_sig\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat Twitter data and Weibo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter=pd.read_csv(r'./Data/interim/twitter_nation_cleaned.csv')\n",
    "data_weibo=pd.read_csv(r'./Data/interim/weibo_nation_cleaned.csv')\n",
    "data_all=pd.concat([data_twitter,data_weibo], axis=1)\n",
    "data_all.to_csv('../Data/interim/all_data_preprocessed.csv', encoding=\"utf_8_sig\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
