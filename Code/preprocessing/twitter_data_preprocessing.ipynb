{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'../../Data/input/Twitter_uncleaned_data.csv')\n",
    "# Extract posting year\n",
    "data['time']= pd.to_datetime(data['ÂèëÂ∏ÉÊó∂Èó¥'],format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "data['year'] = data['time'].year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of duplicate data\n",
    "data=data.drop_duplicates(subset=['Êé®ÊñáÈìæÊé•'],keep='last')\n",
    "# Lowercase the data\n",
    "data['text_cleaned'] = data['ÊñáÂ≠óÂÜÖÂÆπ'].apply(lambda x: x.lower())\n",
    "# Filtering data containing related words\n",
    "keywords = ['urban air mobility', 'uam ','air taxi','evtol','urban aerial mobility','drone taxi','autonomous air vehicle','vtol','flying car','mobility','uav','advenced air','eve','ehang','jaunt','joby','aviation','volocopter']\n",
    "data = data[data[\"text_cleaned\"].str.contains('|'.join(keywords))]\n",
    "# Remove useless characters from text\n",
    "data['text_cleaned'] = data['ÊñáÂ≠óÂÜÖÂÆπ'].str.replace('\\n', ' ')\n",
    "data['text_cleaned'] = data['text_cleaned'].apply(lambda x: re.sub(r'@(\\w+)', '', x))\n",
    "data['text_cleaned'] = data['text_cleaned'].apply(lambda x: re.sub(r'#(\\w+)', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Karlols_ @EP_MrNewport You are so good in wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_cleaned\n",
       "0  @Karlols_ @EP_MrNewport You are so good in wha..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"@Karlols_ @EP_MrNewport You are so good in what you do that makes other people to look after you, even if it is just for words and advice üíñ I'm so damn fing proud of you for not giving up and enjoying life as it goes, I admire your free spirit way to live and your all time smile facey üíñ I love you üíñ\"\n",
    "text1=\"@Karlols_ @EP_MrNewport Hahahah i was just taking the photo and you were the only who noticed hahaha\"\n",
    "data = {'text_cleaned': [text,text1]}\n",
    "data = pd.DataFrame(data)\n",
    "keywords = ['urban air mobility', 'uam ','air taxi','evtol','urban aerial mobility','drone taxi','autonomous air vehicle','vtol ','flying car','mobility','uav ','advenced air','eve ','ehang','jaunt','joby','aviation ','volocopter']\n",
    "data = data[data[\"text_cleaned\"].str.contains('|'.join(keywords))]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword \"VTOL\" data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vtol=data[data['ÂÖ≥ÈîÆËØç']=='vtol']\n",
    "keywords = ['enemy','wars',' war','war ','$vtol','army','weapon','battle','military','soldier']\n",
    "data_vtol=data_vtol[-(data_vtol[\"text_cleaned\"].str.contains('|'.join(keywords)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[-(data['ÂÖ≥ÈîÆËØç']=='vtol')]\n",
    "data=pd.concat([data,data_vtol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nation identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call openai to recognize the country\n",
    "openai.api_key = \"sk-uZNblLpNRmFyvzStF39f4d9d5353430a8792E1134bB95f24\"\n",
    "openai.base_url = \"https://api.gpt.ge/v1/\"\n",
    "openai.default_headers = {\"x-foo\": \"true\"}\n",
    "\n",
    "def askquestion(location):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Please identify the full name of the country represented by each element of the list (location or latitude/longitude) and return a list of the same length, making sure to follow the format ['xxx','xxx','xxx'] strictly, or answer unkonwn if it's not clear, and also answer unkonwn if there are more than one country contained in a single element\"},\n",
    "            {\"role\": \"user\", \"content\": location},\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean nation text\n",
    "data['location']=data[\"Âçö‰∏ª‰ΩçÁΩÆ\"].copy()\n",
    "data['location'].fillna('null', inplace=True)\n",
    "filtered_rows = data['location'].str.match(r'^[0-9. +-]+$')\n",
    "data.loc[filtered_rows, 'location'] = 'useless'\n",
    "keywords = ['World', '.com','world','earth','Global','global','Earth']\n",
    "filtered_rows =data['location'].str.contains('|'.join(keywords), case=False)\n",
    "data.loc[filtered_rows, 'location'] = 'useless'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all=[]\n",
    "a=10\n",
    "\n",
    "for i in range(0, int(len(data)/a)):\n",
    "    tag=True\n",
    "    count=0\n",
    "    while tag and count<=5:\n",
    "        try:\n",
    "            dic={}\n",
    "            dic['No.']=i*a\n",
    "            # Âà†Èô§ÊØè‰∏™ÂÖÉÁ¥†‰∏≠ÁöÑÁ¨¶Âè∑\n",
    "            input_list=data['location'].iloc[i*a:i*a+a].tolist()\n",
    "            cleaned_list = [address.replace(\"'\", '') for address in input_list]\n",
    "            input_location = str(cleaned_list)\n",
    "            dic['input']=input_location\n",
    "            dic['input_count']=len(input_list)\n",
    "            nation=askquestion(input_location)\n",
    "            dic['output']=nation\n",
    "            output_nation = ast.literal_eval(nation)    # Â∞ÜÊñáÊú¨ËΩ¨Êç¢‰∏∫ÂàóË°®\n",
    "            dic['output_count']=len(output_nation)\n",
    "            print(str(i)+'/'+str(len(data)/a))\n",
    "            fp = open(f\"././Data/interim/twitter_nation.csv\", 'a+', encoding='utf-8-sig', newline='')\n",
    "            csv_fp = csv.writer(fp)\n",
    "            csv_fp.writerow(dic.values())\n",
    "            fp.close()\n",
    "            tag=False\n",
    "        except Exception as e:\n",
    "            print(str(i)+'Âá∫Èîô‰∫Ü')\n",
    "            tag=True\n",
    "            count=count+1\n",
    "\n",
    "# After calling openai for recognition, some recognition errors need to be manually checked and corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nation_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nation=pd.read_csv(r'../../Data/interim/twitter_nation.csv')\n",
    "merged_list = []\n",
    "data_nation['output_list'].apply(lambda y:merged_list.extend(ast.literal_eval(y)))\n",
    "my_series = pd.Series(merged_list)\n",
    "nation_list = pd.DataFrame()\n",
    "nation_list['nation'] = my_series\n",
    "# Add nation to all twitter data\n",
    "data_final=pd.concat([data,nation_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the data\n",
    "data_final['nation_cleaned'] = data_final['nation'].apply(lambda x: x.lower())\n",
    "#nation clean\n",
    "data_final['nation_cleaned'].replace(['undefined','country unknown','erusea',' unknown','africa','greenland','unbekannt','unnonwn','bikini bottom','international waters','open skies','sky','belka','lesotho','east timor','international space station','western sahara','guam','un','aus','uam','eu','udknown','metaverse','known','undef','north'], 'unknown', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['undefined','unkonwn','international','european union','unkown', 'unnown','europe','mars','unavailable','antarctica','earth','unknonwn','unknwon','asia','unlnown','unknonw','multiple countries','unidentified','world'], 'unknown', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['united state',' united states','san francisco','united states ','wisconsin','new jersey','united states.','northern mariana islands','us','washington','usa','united sates','united states of america','puerto rico','united states and united kingdom','texas','new hampshire','north carolina','united states virgin islands'], 'united states', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['montserrat','falkland islands',' united kingdom','england','uk','scotland','isle of man','wales','gibraltar','guernsey','jersey','cayman islands','anguilla','saint helena'], 'united kingdom', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['hong kong','taiwan','macau','macao','peoples republic of china'], 'china', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['vatican city','vatican'], 'holy see', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['the netherlands','nederland','aruba','saint martin','sint maarten','south netherlands','Cura√ßao','cura√ßao'], 'netherlands', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['republic of korea'], 'south korea', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['kosovo'], 'serbia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['new caledonia'], 'france', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['democratic republic of the congo','republic of the congo'], 'congo', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['republic of the philippines'], 'philippines', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['the bahamas'], 'bahamas', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['brasil'], 'brazil', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['republic of croatia'], 'croatia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['islamic republic of iran'], 'iran', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['canada and united states','canada '], 'canada', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['ivory coast'], \"c√¥te d'ivoire\", inplace=True)\n",
    "data_final['nation_cleaned'].replace(['kingdom of morocco'], 'morocco', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['russian federation'], 'russia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['lebannon'], 'Lebanon', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['kingdom of saudi arabia'], 'saudi arabia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['federated states of micronesia'], 'micronesia', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['germany '], 'germany', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['islamic republic of pakistan'], 'pakistan', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['nation_clean'] = data_final['nation_clean'].str.title()\n",
    "data_final['nation_cleaned'].replace(['United States'], 'United States of America', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['Antigua And Barbuda'], 'Antigua and Barbuda', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['Bosnia And Herzegovina'], 'Bosnia and Herzegovina', inplace=True)\n",
    "data_final['nation_cleaned'].replace(['Trinidad And Tobago'], 'Trinidad and Tobago', inplace=True)\n",
    "data_final['nation_cleaned'].replace([\"c√¥te d'ivoire\"], \"C√¥te D'Ivoire\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('../../Data/interim/twitter_nation_cleaned.csv', encoding=\"utf_8_sig\",index=True)\n",
    "# After the initial cleaning of the nation text using the code, there were some minor errors that needed to be cleaned manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retain useful data for next statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'././Data/interim/twitter_nation_cleaned.csv')\n",
    "data_retained = data[['ÂÖ≥ÈîÆËØç', 'Êé®ÊñáÈìæÊé•', 'Âçö‰∏ªIDÂè∑','Ë¥¶Êà∑È™åËØÅÁ±ªÂûã','nation_cleaned','year','text_cleaned']]\n",
    "useful_data = pd.DataFrame(data_retained)\n",
    "# Regularization of column names\n",
    "new_column_names = {'ÂÖ≥ÈîÆËØç': 'keyword', 'Êé®ÊñáÈìæÊé•': 'link', 'Âçö‰∏ªIDÂè∑': 'user_id', 'Ë¥¶Êà∑È™åËØÅÁ±ªÂûã': 'user_type', 'nation_cleaned': 'nation', 'year': 'year', 'text_cleaned': 'text_cleaned'}\n",
    "useful_data = useful_data.rename(columns=new_column_names)\n",
    "useful_data.to_csv('../../Data/interim/twitter_text.csv', encoding=\"utf_8_sig\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat Twitter data and Weibo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter=pd.read_csv(r'././Data/interim/twitter_nation_cleaned.csv')\n",
    "data_weibo=pd.read_csv(r'././Data/interim/weibo_nation_cleaned.csv')\n",
    "data_all=pd.concat([data_twitter,data_weibo], axis=1)\n",
    "data_all.to_csv('../../Data/interim/all_data_preprocessed.csv', encoding=\"utf_8_sig\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprogress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
